\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{mathtools}
\setlength{\parindent}{0pt}
\begin{document}

\title{IVR Assignment}
\author{Joseph Abboud and Marley Adair}
\date{}
\maketitle

Joseph Abboud worked on implementing the robot control section. Marley Adair worked on implementing the robot vision section. The project is available on github at \url{https://github.com/gemdude46/ivr_assignment}. \\

\section{Robot Vision}

\subsection{Joint state estimation}

In order to determine the joint states, the program takes the coloured sections for each joint colour from both viewpoints, and uses their moments to determine where they are centred from both perspectives. Then, a ray is cast from each camera out from the centre, and
the closest approach between the lines is used as the coordinates of that joint. In the event the joint is obscured from the perspective of one camera, the rays of all other objects the joint could be obscured behind are tested, and the one with the closest approach
is used. This, however, can still cause errors when the joint is obscured by something that is not a target, such as the connections between joints. \\

After finding the positions of the joints, the angle between the links can be used to find the actual values passed to the robot controller. Figure \ref{fig:visionjointplot} shows the outputs of this algorithm compared to the actual values sent to the joints. Note that
the estimates may be shifted in time due to the latency of processing the images. \\

\begin{figure}
	\makebox[\textwidth][c]{\includegraphics[width=19cm]{visionjointplot.png}}
	\caption{Joint values over time.}
	\label{fig:visionjointplot}
\end{figure}

\subsection{Target detection}

In order to seperate the target from the box, the contours of the orange sections are taken. To determine which one is the target, the shape of the bounding box of each contour is used, and the closest to square is the target. While this works the majority of the time,
there are occations, such as when the target is occluded by a link such that it is split into two long thin sections, when this will incorrectly pick the wrong target. Figure \ref{fig:visiontargetplot} shows the actual and calculated positions of the target. \\

\begin{figure}[h]
	\makebox[\textwidth][c]{\includegraphics[width=\linewidth]{visiontargetplot.png}}
	\caption{Target position over time.}
	\label{fig:visiontargetplot}
\end{figure}

\section{Robot Control}

\subsection{Forward Kinematics}

For the Forward Kinematics of the robot, we see that there are 3 translations along the z-axis, and 4 different rotations (2 about $x$, 1 about $y$ and 1 about $z$), which goes from the base of the robot to the end effector. All these homogeneous transformations matrices  are computed in order give us the final FK matrix, and we get: \\
\begin{center}
    $\begin{bmatrix}
        x_e\\
        y_e\\
        z_e
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        3(S_1C_2S_4 + C_1S_3C_4 + S_1S_2C_3C_4) + 3.5(C_1S_3 + S_1S_2C_3)\\
        3(S_1S_3C_4 - C_1C_2S_4 - C_1C_2C_3C_4) + 3.5(S_1S_3 - C_1S_2C_3)\\
        3(C_2C_3C_4 - S_2S_4) + 3.5C_2C_3 + 2.5
    \end{bmatrix}
    $
\end{center}

Where S is short for sin(), C for cos(), and the index represents joint the angle of rotation $\theta_i$ of joint $i$ (So for example $S_1 = sin(\theta_1))$. So we can say that the FK is given such that: \\ 
\begin{center}
    $\begin{bmatrix}
        x_e\\
        y_e\\
        z_e
    \end{bmatrix}$ 
    = K(q), where q = 
    $\begin{bmatrix}
        \theta_1 \\
        \theta_2 \\
        \theta_3 \\
        \theta_4
    \end{bmatrix}
    $
\end{center}

Figure 3 compares the estimated end effector position from the images and the estimated end effector position from the FK. The inaccuracies can be due to the fact that we have used a lot of trigonometric functions in the FK matrix, which combined all together lead to differences in the computed results,
since floating point numbers are never 100\% accurate. Another issue that could lead to inaccuracies is the joint angles which are estimated, and then fed to the FK matrix, so their accuracy might not always be the on point,
then leading to differences between both end effector estimations. Finally, the inaccuracies can also come from the method used to estimate the end effector position via the images, since we are triangulating a 3D position from two 2D images. \\

\textbf{\textit{Insert Figure.3}}

\subsection{Closed\_loop Control}
This task designs a closed\_loop controller for the robot. The program calculates the robot's Jacobian $J$ by deriving $(x_e, y_e, z_e)$ with respect to $(\theta_1, \theta_2, \theta_3, \theta_4)$. So $J$ has dimensions $3\times4$, and $J =$

\begin{multline}
 \left[
      \begin{matrix}
        3(C_1C_2S_4 - S_1S_3C_4 + C_1S_2C_3C_4) + 3.5(C_1S_2C_3 - S_1S_3)\\ 
        3(S_1C_2S_4 + C_1S_3C_4 + S_1C_2C_3C_4) + 3.5(C_1S_3 + S_1S_2C_3)\\ 
        0\\
      \end{matrix}\right.                
    \\
    \begin{matrix}
        3(S_1C_2C_3C_4 - S_1S_2S_4) + 3.5(S_1C_2C_3)\\ 
        3(C_1S_2S_4 + C_1S_2C_3C_4) - 3.5C_1C_2C_3\\ 
        -3(C_2S_4 + S_2C_3C_4) -3.5S_2C_3\\
      \end{matrix}
      \\
      \begin{matrix}
        3(C_1C_3C_4 - S_1S_2S_3C_4) + 3.5(C_1C_3 - S_1S_2S_3)\\ 
        3(S_1C_3C_4 + C_1C_2S_3C_4) + 3.5(S_1C_3 + C_1S_2C_3)\\ 
        -3C_2S_3C_4 -3.5C_2S_3\\
      \end{matrix}
      \\
      \left.
      \begin{matrix}
        3(S_1C_2C_4 - C_1S_3S_4 - S_1S_2C_3S_4)\\ 
        3(C_1C_2C_3S_4 - C_1C_2C_4 - S_1S_3S_4)\\ 
        -3(S_2C_4 + C_2C_3S_4)\\
      \end{matrix}\right]
\end{multline}
With the Jacobian computed, the program tries to follow the target using an adaptation of the closed loop control method used in Lab3 earlier in the semester. The plots in Figure 4 help us see the difference between the computed position the robot tries to follow, and the true position of the target. 

\textbf{\textit{Insert Figure.4}}

\end{document}
